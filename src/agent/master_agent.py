# src/agent/master_agent.py
import logging
from openai import OpenAI
import config
from src.memory.long_term_memory import LongTermMemory
from datetime import datetime
import json
import re

logger = logging.getLogger(__name__)

class MasterAgent:
    def __init__(self, memory: LongTermMemory):
        self.memory = memory
        self.llm_client = OpenAI(api_key=config.LLM_API_KEY, base_url=config.LLM_BASE_URL)
        logger.info("MasterAgent initialized.")

    # --- ä¸“å®¶æ–¹æ³• (ç§æœ‰) ---

    def _get_query_route(self, query: str) -> str:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†æä¸è·¯ç”±ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æœ€é€‚åˆå¤„ç†è¯¥é—®é¢˜çš„ä¸“å®¶ã€‚
å¯ç”¨ä¸“å®¶å¦‚ä¸‹ï¼š
- `memory_retrieval`: å½“é—®é¢˜æ˜¯å…³äºå…·ä½“çš„ã€è¿‘æœŸå‘ç”Ÿçš„äº‹ä»¶ï¼Œéœ€è¦å›å¿†æŸä¸ªåœºæ™¯æˆ–åŠ¨ä½œæ—¶ã€‚ä¾‹å¦‚ï¼š"æˆ‘åˆšæ‰æŠŠé’¥åŒ™æ”¾å“ªäº†ï¼Ÿ", "lizhijunä¸‹åˆåœ¨åšä»€ä¹ˆï¼Ÿ"
- `graph_reasoning`: å½“é—®é¢˜æ˜¯å…³äºå®ä½“ä¹‹é—´çš„å…³ç³»ã€è§„å¾‹ã€é¢‘ç‡æˆ–ç»Ÿè®¡æ—¶ï¼Œéœ€è¦è¿›è¡Œé€»è¾‘æ¨ç†ã€‚ä¾‹å¦‚ï¼š"æˆ‘å’Œè°ä¸€èµ·å‡ºç°è¿‡ï¼Ÿ", "æˆ‘çš„æ¯å­é€šå¸¸æ”¾åœ¨å“ªé‡Œï¼Ÿ"
- `summarization`: å½“é—®é¢˜è¦æ±‚å¯¹ä¸€æ®µæ—¶é—´çš„æ´»åŠ¨è¿›è¡Œæ€»ç»“æˆ–å›é¡¾æ—¶ã€‚ä¾‹å¦‚ï¼š"æ€»ç»“æˆ‘ä»Šå¤©ä¸Šåˆçš„æ´»åŠ¨", "ä¸Šå‘¨æˆ‘éƒ½å¹²äº†äº›ä»€ä¹ˆï¼Ÿ"
ç”¨æˆ·é—®é¢˜: "{query}"
è¯·åªè¿”å›æœ€åˆé€‚çš„ä¸“å®¶åç§° (memory_retrieval, graph_reasoning, summarization)ã€‚
"""
        try:
            response = self.llm_client.chat.completions.create(model=config.LLM_MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.0)
            route = response.choices[0].message.content.strip().lower()
            valid_routes = ["memory_retrieval", "graph_reasoning", "summarization"]
            return route if route in valid_routes else "memory_retrieval"
        except Exception as e:
            logger.error(f"Routing failed: {e}")
            return "memory_retrieval"

    def _refine_query_for_retrieval(self, query: str) -> str:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚ç”¨æˆ·çš„åŸå§‹é—®é¢˜å¯èƒ½å¾ˆå£è¯­åŒ–ã€‚è¯·å°†å…¶æ”¹å†™æˆä¸€ä¸ªæ›´é€‚åˆå‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢çš„é™ˆè¿°å¥æˆ–å…³é”®è¯ç»„ã€‚
åŸå§‹é—®é¢˜: "{query}"
ä¼˜åŒ–åçš„æŸ¥è¯¢:
"""
        try:
            response = self.llm_client.chat.completions.create(model=config.LLM_MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.0)
            return response.choices[0].message.content.strip()
        except Exception:
            return query

    def _extract_entities_from_query(self, query: str) -> list:
        prompt = f"""
ä»ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜ä¸­æå–å‡ºæ ¸å¿ƒçš„å®ä½“ï¼ˆç‰¹åˆ«æ˜¯äººåã€ç‰©ä½“åï¼‰ã€‚
ç”¨æˆ·é—®é¢˜: "{query}"
è¯·ä»¥Pythonåˆ—è¡¨çš„æ ¼å¼è¿”å›æå–åˆ°çš„å®ä½“ï¼Œä¾‹å¦‚: ["lizhijun", "æ¯å­"]
å¦‚æœæ‰¾ä¸åˆ°å®ä½“ï¼Œè¯·è¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨: []
"""
        try:
            response = self.llm_client.chat.completions.create(model=config.LLM_MODEL_NAME, messages=[{"role": "user", "content": prompt}], temperature=0.0)
            content = response.choices[0].message.content
            match = re.search(r'\[(.*?)\]', content)
            if match:
                entities = eval(f"[{match.group(1)}]")
                if isinstance(entities, list):
                    return entities
            return []
        except Exception as e:
            logger.error(f"Failed to extract entities from query: {e}")
            return query.split()

    def _extract_time_period(self, query: str):
        now = datetime.now()
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ—¶é—´è§£æä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œå½“å‰æ—¶é—´ï¼Œæå–ä¸€ä¸ªå¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´ã€‚
å½“å‰æ—¶é—´: {now.isoformat()}
ç”¨æˆ·é—®é¢˜: "{query}"
è¯·ä»¥JSONæ ¼å¼è¿”å›: {{"start_iso": "YYYY-MM-DDTHH:MM:SS", "end_iso": "YYYY-MM-DDTHH:MM:SS"}}
"""
        try:
            response = self.llm_client.chat.completions.create(
                model=config.LLM_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            time_data = json.loads(response.choices[0].message.content)
            start_dt = datetime.fromisoformat(time_data['start_iso'])
            end_dt = datetime.fromisoformat(time_data['end_iso'])
            return start_dt.timestamp(), end_dt.timestamp()
        except Exception:
            today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            return today_start.timestamp(), now.timestamp()

    # --- ä¸“å®¶æ–¹æ³• (ä¿®æ­£ return ä¸º yield) ---

    def _memory_retrieval_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `è®°å¿†æ£€ç´¢ä¸“å®¶`\n")
        
        yield "**å†³ç­–è·¯å¾„**: `è®°å¿†æ£€ç´¢`\næ­£åœ¨æå–é—®é¢˜ä¸­çš„æ ¸å¿ƒå®ä½“... ğŸ”"
        entities = self._extract_entities_from_query(query)
        # å¦‚æœå¤§æ¨¡å‹æå–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„åˆ†è¯ä½œä¸ºåå¤‡
        if not entities:
             entities = [kw for kw in query.replace("åœ¨åšä»€ä¹ˆ", "").replace("çš„", "").split() if kw]
        
        streamer(f"**æå–åˆ°å®ä½“**: `{entities}`\n")
        yield f"**å†³ç­–è·¯å¾„**: `è®°å¿†æ£€ç´¢`\n**æå–åˆ°å®ä½“**: `{entities}`\næ­£åœ¨è¿›è¡Œæ··åˆæœç´¢ (è¯­ä¹‰+å…³é”®è¯)... ğŸ§ "

        refined_query = self._refine_query_for_retrieval(query)
        semantic_results = self.memory.semantic_search(refined_query, top_k=3)
        
        # å…³é”®è¯æœç´¢éœ€è¦åˆ—è¡¨
        keyword_results = self.memory.keyword_search(entities, top_k=3)
        
        all_events = {}
        for event in semantic_results + keyword_results:
            all_events[event['event_id']] = event
            
        retrieved_events = sorted(all_events.values(), key=lambda x: x['start_time'], reverse=True)
        
        if not retrieved_events:
            # !!! ä¿®æ­£ï¼šyield None !!!
            yield None
            return
        
        streamer(f"**æ£€ç´¢åˆ° {len(retrieved_events)} æ¡ç›¸å…³è®°å¿†**\n")
        
        context_str = "ä»¥ä¸‹æ˜¯æˆ‘æ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†ç‰‡æ®µï¼š\n\n"
        for i, event in enumerate(retrieved_events):
            time_str = datetime.fromtimestamp(event['start_time']).strftime('%Y-%m-%d %H:%M:%S')
            context_str += f"--- è®°å¿†ç‰‡æ®µ {i+1} [{time_str}] ---\næ‘˜è¦: {event['summary']}\n\n"
            
        # !!! ä¿®æ­£ï¼šyield ç»“æœå…ƒç»„ !!!
        yield (context_str, retrieved_events)

    def _graph_reasoning_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `çŸ¥è¯†å›¾è°±æ¨ç†ä¸“å®¶`\n")
        yield "**å†³ç­–è·¯å¾„**: `çŸ¥è¯†å›¾è°±æ¨ç†`\næ­£åœ¨å°†æ‚¨çš„é—®é¢˜è½¬æ¢ä¸ºæ•°æ®åº“æŸ¥è¯¢... âš™ï¸"
        
        result = self.memory.query_knowledge_graph_by_nl(query)
        streamer(f"**çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ**: \n{result}\n")
        
        context_str = f"ä»¥ä¸‹æ˜¯æˆ‘ä»çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢åˆ°çš„ä¿¡æ¯ï¼š\n\n{result}"
        # !!! ä¿®æ­£ï¼šyield ç»“æœå…ƒç»„ !!!
        yield (context_str, None)

    def _summarization_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `æ´»åŠ¨æ€»ç»“ä¸“å®¶`\n")
        yield "**å†³ç­–è·¯å¾„**: `æ´»åŠ¨æ€»ç»“`\næ­£åœ¨è§£ææ‚¨é—®é¢˜ä¸­çš„æ—¶é—´èŒƒå›´... ğŸ“…"
        
        start_ts, end_ts = self._extract_time_period(query)
        start_dt_str = datetime.fromtimestamp(start_ts).strftime('%Y-%m-%d %H:%M')
        end_dt_str = datetime.fromtimestamp(end_ts).strftime('%Y-%m-%d %H:%M')
        streamer(f"**è§£ææ—¶é—´èŒƒå›´**: `{start_dt_str}` åˆ° `{end_dt_str}`\n")
        yield f"**å†³ç­–è·¯å¾„**: `æ´»åŠ¨æ€»ç»“`\n**æ—¶é—´èŒƒå›´**: `{start_dt_str}` -> `{end_dt_str}`\næ­£åœ¨ä»æ•°æ®åº“ç­›é€‰äº‹ä»¶... ğŸ“š"
        
        events = self.memory.get_events_for_period(start_ts, end_ts)
        
        if not events:
             # !!! ä¿®æ­£ï¼šyield ç»“æœå…ƒç»„ !!!
            yield (f"åœ¨ {start_dt_str} åˆ° {end_dt_str} æœŸé—´æ²¡æœ‰å‘ç°ä»»ä½•è®°å¿†è®°å½•ã€‚", None)
            return
            
        context_str = f"ä»¥ä¸‹æ˜¯ä» {start_dt_str} åˆ° {end_dt_str} æœŸé—´ï¼ŒæŒ‰æ—¶é—´é¡ºåºè®°å½•çš„æ‰€æœ‰æ´»åŠ¨æ‘˜è¦ï¼š\n\n"
        for event in events:
            event_time = datetime.fromtimestamp(event['start_time']).strftime('%Y-%m-%d %H:%M')
            context_str += f"- [{event_time}] {event['summary']}\n"
            
        # !!! ä¿®æ­£ï¼šyield ç»“æœå…ƒç»„ !!!
        yield (context_str, None)

    def _generate_final_answer(self, query, context):
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªAI Agentçš„è®°å¿†æ ¸å¿ƒã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‘ï¼Œç”¨è‡ªç„¶ã€æµç•…çš„å£å»å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå°±å¦è¯šè¯´æ˜ã€‚"
        user_prompt = f"ã€èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{query}"
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        try:
            response = self.llm_client.chat.completions.create(model=config.LLM_MODEL_NAME, messages=messages, stream=True, temperature=0.5)
            for chunk in response:
                if chunk.choices[0].delta and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {e}"

    # --- ä¸»æ‰§è¡Œæµç¨‹ (å…¬å…±) ---
    def execute_query_steps(self, query, streamer):
        streamer("æ­£åœ¨åˆ†æé—®é¢˜...\n")
        yield {"status": "routing", "content": "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜... ğŸ¤”"}
        route = self._get_query_route(query)
        streamer(f"å†³ç­–è·¯å¾„: {route}\n")
        yield {"status": "routing", "content": f"**å†³ç­–è·¯å¾„**: `{route}`"}

        context, evidence = None, None
        context_generator = None

        if route == "memory_retrieval":
            context_generator = self._memory_retrieval_expert(query, streamer)
        elif route == "graph_reasoning":
            context_generator = self._graph_reasoning_expert(query, streamer)
        elif route == "summarization":
            context_generator = self._summarization_expert(query, streamer)
        
        if context_generator:
            # æ¶ˆè´¹ç”Ÿæˆå™¨
            for step_output in context_generator:
                if isinstance(step_output, str): # ä¸­é—´çŠ¶æ€æ›´æ–°
                    yield {"status": "retrieving", "content": step_output}
                elif isinstance(step_output, tuple): # æœ€ç»ˆç»“æœ (context, evidence)
                    context, evidence = step_output
                elif step_output is None: # æ²¡æ‰¾åˆ°ç»“æœ
                     context, evidence = None, None

        if not context:
            yield {"status": "done", "content": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"}
            return

        yield {"status": "generating", "content": f"**å†³ç­–è·¯å¾„**: `{route}`\nä¿¡æ¯æ£€ç´¢å®Œæ¯•ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”... âœï¸"}

        final_answer = ""
        for chunk in self._generate_final_answer(query, context):
            final_answer += chunk
            yield {"status": "generating", "content": final_answer, "evidence": evidence}

        yield {"status": "done", "content": final_answer, "evidence": evidence}