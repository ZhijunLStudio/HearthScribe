# src/agent/master_agent.py (çœŸæ­£å®Œæ•´çš„æœ€ç»ˆç‰ˆ)
import logging
from openai import OpenAI
import config
from src.memory.long_term_memory import LongTermMemory
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class MasterAgent:
    def __init__(self, memory: LongTermMemory):
        self.memory = memory
        self.llm_client = OpenAI(api_key=config.LLM_API_KEY, base_url=config.LLM_BASE_URL)
        logger.info("MasterAgent initialized.")

    # --- ä¸“å®¶æ–¹æ³• (ç§æœ‰) ---

    def _get_query_route(self, query: str) -> str:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†æä¸è·¯ç”±ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æœ€é€‚åˆå¤„ç†è¯¥é—®é¢˜çš„ä¸“å®¶ã€‚
å¯ç”¨ä¸“å®¶å¦‚ä¸‹ï¼š
- `memory_retrieval`: å½“é—®é¢˜æ˜¯å…³äºå…·ä½“çš„ã€è¿‘æœŸå‘ç”Ÿçš„äº‹ä»¶ï¼Œéœ€è¦å›å¿†æŸä¸ªåœºæ™¯æˆ–åŠ¨ä½œæ—¶ã€‚ä¾‹å¦‚ï¼š"æˆ‘åˆšæ‰æŠŠé’¥åŒ™æ”¾å“ªäº†ï¼Ÿ", "lizhijunä¸‹åˆåœ¨åšä»€ä¹ˆï¼Ÿ"
- `graph_reasoning`: å½“é—®é¢˜æ˜¯å…³äºå®ä½“ä¹‹é—´çš„å…³ç³»ã€è§„å¾‹ã€é¢‘ç‡æˆ–ç»Ÿè®¡æ—¶ï¼Œéœ€è¦è¿›è¡Œé€»è¾‘æ¨ç†ã€‚ä¾‹å¦‚ï¼š"æˆ‘å’Œè°ä¸€èµ·å‡ºç°è¿‡ï¼Ÿ", "æˆ‘çš„æ¯å­é€šå¸¸æ”¾åœ¨å“ªé‡Œï¼Ÿ"
- `summarization`: å½“é—®é¢˜è¦æ±‚å¯¹ä¸€æ®µæ—¶é—´çš„æ´»åŠ¨è¿›è¡Œæ€»ç»“æˆ–å›é¡¾æ—¶ã€‚ä¾‹å¦‚ï¼š"æ€»ç»“æˆ‘ä»Šå¤©ä¸Šåˆçš„æ´»åŠ¨", "ä¸Šå‘¨æˆ‘éƒ½å¹²äº†äº›ä»€ä¹ˆï¼Ÿ"

ç”¨æˆ·é—®é¢˜: "{query}"

è¯·åªè¿”å›æœ€åˆé€‚çš„ä¸“å®¶åç§° (memory_retrieval, graph_reasoning, summarization)ã€‚
"""
        response = self.llm_client.chat.completions.create(
            model=config.LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        route = response.choices[0].message.content.strip().lower()
        valid_routes = ["memory_retrieval", "graph_reasoning", "summarization"]
        return route if route in valid_routes else "memory_retrieval"

    def _refine_query_for_retrieval(self, query: str) -> str:
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚ç”¨æˆ·çš„åŸå§‹é—®é¢˜å¯èƒ½å¾ˆå£è¯­åŒ–ã€‚è¯·å°†å…¶æ”¹å†™æˆä¸€ä¸ªæ›´é€‚åˆå‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢çš„é™ˆè¿°å¥æˆ–å…³é”®è¯ç»„ã€‚

åŸå§‹é—®é¢˜: "{query}"
ä¼˜åŒ–åçš„æŸ¥è¯¢:
"""
        response = self.llm_client.chat.completions.create(
            model=config.LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()

    def _extract_time_period(self, query: str):
        now = datetime.now()
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ—¶é—´è§£æä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œå½“å‰æ—¶é—´ï¼Œæå–ä¸€ä¸ªå¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´ã€‚
å½“å‰æ—¶é—´: {now.isoformat()}

ç”¨æˆ·é—®é¢˜: "{query}"

è¯·ä»¥JSONæ ¼å¼è¿”å›: {{"start_iso": "YYYY-MM-DDTHH:MM:SS", "end_iso": "YYYY-MM-DDTHH:MM:SS"}}
"""
        try:
            response = self.llm_client.chat.completions.create(
                model=config.LLM_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            time_data = json.loads(response.choices[0].message.content)
            start_dt = datetime.fromisoformat(time_data['start_iso'])
            end_dt = datetime.fromisoformat(time_data['end_iso'])
            return start_dt.timestamp(), end_dt.timestamp()
        except Exception:
            today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            return today_start.timestamp(), now.timestamp()
            
    def _memory_retrieval_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `è®°å¿†æ£€ç´¢ä¸“å®¶`\n")
        yield "**å†³ç­–è·¯å¾„**: `è®°å¿†æ£€ç´¢`\næ­£åœ¨ä¼˜åŒ–æ‚¨çš„é—®é¢˜... ğŸ§"
        
        refined_query = self._refine_query_for_retrieval(query)
        streamer(f"**ä¼˜åŒ–æŸ¥è¯¢**: `{query}` -> `{refined_query}`\n")
        yield f"**å†³ç­–è·¯å¾„**: `è®°å¿†æ£€ç´¢`\n**ä¼˜åŒ–æŸ¥è¯¢**: `{refined_query}`\næ­£åœ¨è¿›è¡Œå‘é‡æœç´¢... ğŸ§ "

        retrieved_events = self.memory.semantic_search(refined_query, top_k=4)
        
        if not retrieved_events:
            return None, None
        
        context_str = "ä»¥ä¸‹æ˜¯æˆ‘æ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†ç‰‡æ®µï¼š\n\n"
        for i, event in enumerate(retrieved_events):
            time_str = datetime.fromtimestamp(event['start_time']).strftime('%Y-%m-%d %H:%M:%S')
            context_str += f"--- è®°å¿†ç‰‡æ®µ {i+1} [{time_str}] ---\næ‘˜è¦: {event['summary']}\n\n"
            
        return context_str, retrieved_events

    def _graph_reasoning_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `çŸ¥è¯†å›¾è°±æ¨ç†ä¸“å®¶`\n")
        yield "**å†³ç­–è·¯å¾„**: `çŸ¥è¯†å›¾è°±æ¨ç†`\næ­£åœ¨å°†æ‚¨çš„é—®é¢˜è½¬æ¢ä¸ºæ•°æ®åº“æŸ¥è¯¢... âš™ï¸"
        
        result = self.memory.query_knowledge_graph_by_nl(query)
        streamer(f"**çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ**: \n{result}\n")
        
        context_str = f"ä»¥ä¸‹æ˜¯æˆ‘ä»çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢åˆ°çš„ä¿¡æ¯ï¼š\n\n{result}"
        return context_str, None # KGæŸ¥è¯¢æ²¡æœ‰ç›´æ¥çš„â€œäº‹ä»¶â€è¯æ®

    def _summarization_expert(self, query: str, streamer):
        streamer("**è°ƒç”¨ä¸“å®¶**: `æ´»åŠ¨æ€»ç»“ä¸“å®¶`\n")
        yield "**å†³ç­–è·¯å¾„**: `æ´»åŠ¨æ€»ç»“`\næ­£åœ¨è§£ææ‚¨é—®é¢˜ä¸­çš„æ—¶é—´èŒƒå›´... ğŸ“…"
        
        start_ts, end_ts = self._extract_time_period(query)
        start_dt_str = datetime.fromtimestamp(start_ts).strftime('%Y-%m-%d %H:%M')
        end_dt_str = datetime.fromtimestamp(end_ts).strftime('%Y-%m-%d %H:%M')
        streamer(f"**è§£ææ—¶é—´èŒƒå›´**: `{start_dt_str}` åˆ° `{end_dt_str}`\n")
        yield f"**å†³ç­–è·¯å¾„**: `æ´»åŠ¨æ€»ç»“`\n**æ—¶é—´èŒƒå›´**: `{start_dt_str}` -> `{end_dt_str}`\næ­£åœ¨ä»æ•°æ®åº“ç­›é€‰äº‹ä»¶... ğŸ“š"
        
        events = self.memory.get_events_for_period(start_ts, end_ts)
        
        if not events:
            return f"åœ¨ {start_dt_str} åˆ° {end_dt_str} æœŸé—´æ²¡æœ‰å‘ç°ä»»ä½•è®°å¿†è®°å½•ã€‚", None
            
        context_str = f"ä»¥ä¸‹æ˜¯ä» {start_dt_str} åˆ° {end_dt_str} æœŸé—´ï¼ŒæŒ‰æ—¶é—´é¡ºåºè®°å½•çš„æ‰€æœ‰æ´»åŠ¨æ‘˜è¦ï¼š\n\n"
        for event in events:
            event_time = datetime.fromtimestamp(event['start_time']).strftime('%Y-%m-%d %H:%M')
            context_str += f"- [{event_time}] {event['summary']}\n"
            
        return context_str, None # æ€»ç»“ä»»åŠ¡çš„â€œè¯æ®â€å°±æ˜¯ä¸Šä¸‹æ–‡æœ¬èº«ï¼Œä¸å•ç‹¬å±•ç¤º

    def _generate_final_answer(self, query, context):
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„ç”Ÿæˆå™¨"""
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªAI Agentçš„è®°å¿†æ ¸å¿ƒã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‘ï¼Œç”¨è‡ªç„¶ã€æµç•…çš„å£å»å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå°±å¦è¯šè¯´æ˜ã€‚"
        user_prompt = f"ã€èƒŒæ™¯ä¸Šä¸‹æ–‡ã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{query}"
        
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        response = self.llm_client.chat.completions.create(model=config.LLM_MODEL_NAME, messages=messages, stream=True, temperature=0.5)
        
        for chunk in response:
            if chunk.choices[0].delta and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    # --- ä¸»æ‰§è¡Œæµç¨‹ (å…¬å…±) ---
    def execute_query_steps(self, query, streamer):
        """åˆ†æ­¥æ‰§è¡ŒæŸ¥è¯¢å¹¶yieldä¸­é—´çŠ¶æ€"""
        # 1. è·¯ç”±
        streamer("æ­£åœ¨åˆ†æé—®é¢˜...\n")
        yield {"status": "routing", "content": "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜... ğŸ¤”"}
        route = self._get_query_route(query)
        streamer(f"å†³ç­–è·¯å¾„: {route}\n")
        yield {"status": "routing", "content": f"**å†³ç­–è·¯å¾„**: `{route}`"}

        # 2. è·å–ä¸Šä¸‹æ–‡ (BINGO! å·²ä¸ºæ‰€æœ‰åˆ†æ”¯å®ç°)
        context, evidence = None, None
        context_generator = None

        if route == "memory_retrieval":
            context_generator = self._memory_retrieval_expert(query, streamer)
        elif route == "graph_reasoning":
            context_generator = self._graph_reasoning_expert(query, streamer)
        elif route == "summarization":
            context_generator = self._summarization_expert(query, streamer)
        
        # æ¶ˆè´¹ç”Ÿæˆå™¨å¹¶yieldä¸­é—´çŠ¶æ€
        if context_generator:
            # æ£€æŸ¥å®ƒæ˜¯ä¸æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨
            if hasattr(context_generator, '__iter__') and not isinstance(context_generator, (str, tuple)):
                for step_output in context_generator:
                    if isinstance(step_output, str): # ä¸­é—´çŠ¶æ€æ›´æ–°
                        yield {"status": "retrieving", "content": step_output}
                    else: # æœ€ç»ˆç»“æœ (context, evidence)
                        context, evidence = step_output
            else: # å¦‚æœä¸æ˜¯ç”Ÿæˆå™¨ï¼Œç›´æ¥è·å–ç»“æœ
                context, evidence = context_generator

        if not context:
            yield {"status": "done", "content": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"}
            return

        yield {"status": "generating", "content": f"**å†³ç­–è·¯å¾„**: `{route}`\nä¿¡æ¯æ£€ç´¢å®Œæ¯•ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”... âœï¸"}

        # 3. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = ""
        answer_generator = self._generate_final_answer(query, context)
        for chunk in answer_generator:
            final_answer += chunk
            yield {"status": "generating", "content": final_answer, "evidence": evidence}

        # 4. æœ€ç»ˆå®Œæˆ
        yield {"status": "done", "content": final_answer, "evidence": evidence}