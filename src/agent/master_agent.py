import logging
from openai import OpenAI
import config
from src.memory.long_term_memory import LongTermMemory
from datetime import datetime
import json
import traceback

logger = logging.getLogger(__name__)

class MasterAgent:
    def __init__(self, memory: LongTermMemory):
        self.memory = memory
        # å…¼å®¹ config å†™æ³•ï¼Œä¼˜å…ˆè¯»å– ERNIE é…ç½®
        api_key = getattr(config, 'API_KEY', getattr(config, 'ERNIE_API_KEY', ''))
        base_url = getattr(config, 'BASE_URL', getattr(config, 'ERNIE_BASE_URL', ''))
        
        self.llm_client = OpenAI(api_key=api_key, base_url=base_url)
        logger.info("MasterAgent initialized.")

    def _get_query_route(self, query: str) -> str:
        """
        ç®€å•è·¯ç”±ï¼šåˆ¤æ–­ç”¨æˆ·æ˜¯æƒ³æŸ¥è®°å¿†ã€æŸ¥å›¾è°±è¿˜æ˜¯åšæ€»ç»“ã€‚
        """
        prompt = f"""
        ä»»åŠ¡ï¼šæ„å›¾åˆ†ç±»ã€‚
        é€‰é¡¹ï¼š
        - memory_retrieval: æŸ¥è¯¢å…·ä½“äº‹ä»¶ã€æ‰¾äººã€æ‰¾ä¸œè¥¿ (ä¾‹å¦‚: "å¼ ä¸‰ä»€ä¹ˆæ—¶å€™å›æ¥çš„?", "è°åœ¨å®¢å…?")
        - graph_reasoning: æŸ¥è¯¢ç»Ÿè®¡ã€å…³ç³»ã€é¢‘ç‡ (ä¾‹å¦‚: "æˆ‘å’Œè°äº’åŠ¨æœ€å¤š?", "è°æœ€å¸¸æ¥?")
        - summarization: æ—¶é—´æ®µæ€»ç»“ (ä¾‹å¦‚: "ä»Šå¤©ä¸Šåˆå‘ç”Ÿäº†ä»€ä¹ˆ?", "ç”Ÿæˆæ—¥æŠ¥")
        
        ç”¨æˆ·é—®é¢˜: "{query}"
        
        åªè¿”å›åˆ†ç±»åç§°ï¼Œä¸è¦æ ‡ç‚¹ç¬¦å·ã€‚
        """
        try:
            # ä½¿ç”¨æ€è€ƒæ¨¡å‹è¿›è¡Œè·¯ç”±å†³ç­–
            model_name = getattr(config, 'AI_THINKING_MODEL', 'ernie-4.5-vl-28b-a3b-thinking')
            response = self.llm_client.chat.completions.create(
                model=model_name, 
                messages=[{"role": "user", "content": prompt}], 
                temperature=0.0
            )
            route = response.choices[0].message.content.strip().lower()
            valid_routes = ["memory_retrieval", "graph_reasoning", "summarization"]
            return route if route in valid_routes else "memory_retrieval"
        except Exception as e:
            logger.error(f"Routing error: {e}")
            return "memory_retrieval"

    def _memory_retrieval_expert(self, query: str):
        """è®°å¿†æ£€ç´¢ä¸“å®¶ï¼šè¿”å›æ£€ç´¢åˆ°çš„ Context å­—ç¬¦ä¸²"""
        # 1. å°è¯•è¯­ä¹‰æœç´¢
        if hasattr(self.memory, 'semantic_search'):
            # æœç´¢æœ€ç›¸å…³çš„ 5 æ¡
            results = self.memory.semantic_search(query, top_k=5)
        else:
            # é™çº§æ–¹æ¡ˆ
            results = self.memory.get_rich_event_details(limit=5)
            
        if not results:
            return None, None

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_str = "ã€ç›¸å…³è®°å¿†ç‰‡æ®µã€‘:\n"
        for i, event in enumerate(results):
            t = datetime.fromtimestamp(event['start_time']).strftime('%Y-%m-%d %H:%M:%S')
            # ç¡®ä¿è§£ææ‘˜è¦ä¸­çš„æ ‡ç­¾å’Œè¯„åˆ†
            summary_text = event['summary'].split("|||")[0] 
            context_str += f"- æ—¶é—´: {t} | äº‹ä»¶: {summary_text}\n"
            
        return context_str, results

    def _graph_reasoning_expert(self, query: str):
        """çŸ¥è¯†å›¾è°±ä¸“å®¶"""
        res = self.memory.query_knowledge_graph_by_nl(query)
        context_str = f"ã€çŸ¥è¯†å›¾è°±æ•°æ®ã€‘:\n{res}"
        return context_str, None

    def _summarization_expert(self, query: str):
        """æ€»ç»“ä¸“å®¶"""
        today_start = datetime.now().replace(hour=0, minute=0, second=0).timestamp()
        events = self.memory.get_events_for_period(today_start, datetime.now().timestamp())
        
        if not events:
            return "ä»Šæ—¥æš‚æ— è®°å½•ã€‚", None
            
        context_str = "ã€ä»Šæ—¥æ´»åŠ¨æµæ°´ã€‘:\n"
        for e in events:
            t = datetime.fromtimestamp(e['start_time']).strftime('%H:%M')
            summary_text = e['summary'].split("|||")[0]
            context_str += f"- [{t}] {summary_text}\n"
            
        return context_str, None

    def _generate_final_answer(self, query, context):
        """ç”Ÿæˆæœ€ç»ˆå›ç­” (æµå¼)"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¶åº­ç®¡å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ã€è®°å¿†ä¿¡æ¯ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
        
        è¦æ±‚ï¼š
        1. å¿…é¡»åŸºäºæä¾›çš„è®°å¿†ä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚
        2. å¼•ç”¨å…·ä½“çš„æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚â€œåœ¨14:30åˆ†çš„æ—¶å€™...â€ï¼‰ã€‚
        3. å¦‚æœè®°å¿†ä¸­åŒ…å«å…·ä½“äººåï¼ˆå¦‚å¼ ä¸‰ã€æå››ï¼‰ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼Œä¸è¦åªè¯´â€œæœ‰äººâ€ã€‚
        4. è¯­æ°”è‡ªç„¶ã€äº²åˆ‡ã€‚
        
        {context}
        
        ã€ç”¨æˆ·é—®é¢˜ã€‘: {query}
        """
        try:
            model_name = getattr(config, 'AI_THINKING_MODEL', 'ernie-4.5-vl-28b-a3b-thinking')
            resp = self.llm_client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                stream=True
            )
            for chunk in resp:
                # --- å…³é”®ä¿®å¤ï¼šé˜²æ­¢ list index out of range ---
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
        except Exception as e:
            logger.error(f"Generate answer error: {e}")
            yield f" [ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}]"

    def execute_query_steps(self, query, streamer=None):
        """
        ä¸»æ‰§è¡Œå…¥å£ï¼Œè¿”å›ç”Ÿæˆå™¨ã€‚
        Yields:
            {'status': 'thinking', 'content': '...'}  -> ç”¨äºå‰ç«¯æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            {'status': 'answer', 'content': '...'}    -> ç”¨äºå‰ç«¯æ˜¾ç¤ºæœ€ç»ˆå›ç­”
        """
        # Step 1: è·¯ç”±å†³ç­–
        yield {"status": "thinking", "content": "ğŸ¤” æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜æ„å›¾..."}
        route = self._get_query_route(query)
        yield {"status": "thinking", "content": f"ğŸ‘‰ å†³ç­–è·¯å¾„: `{route}`"}
        
        # Step 2: è°ƒç”¨ä¸“å®¶æ£€ç´¢
        expert_gen = None
        context = None
        
        if route == 'memory_retrieval':
            yield {"status": "thinking", "content": "ğŸ” æ­£åœ¨æ£€ç´¢è¯­ä¹‰è®°å¿†åº“..."}
            context, _ = self._memory_retrieval_expert(query)
        elif route == 'graph_reasoning':
            yield {"status": "thinking", "content": "ğŸ•¸ï¸ æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†å›¾è°±..."}
            context, _ = self._graph_reasoning_expert(query)
        elif route == 'summarization':
            yield {"status": "thinking", "content": "ğŸ“… æ­£åœ¨èšåˆä»Šæ—¥æ´»åŠ¨è®°å½•..."}
            context, _ = self._summarization_expert(query)
        
        if not context:
            yield {"status": "thinking", "content": "âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"}
            yield {"status": "answer", "content": "æŠ±æ­‰ï¼Œæˆ‘åœ¨è®°å¿†ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯ã€‚"}
            return

        yield {"status": "thinking", "content": "âœ… ä¿¡æ¯æ£€ç´¢å®Œæ¯•ï¼Œæ­£åœ¨ç»„ç»‡è¯­è¨€..."}
        
        # Step 3: ç”Ÿæˆå›ç­”
        full_ans = ""
        for chunk in self._generate_final_answer(query, context):
            full_ans += chunk
            # å®æ—¶æµå¼è¾“å‡º
            yield {"status": "answer", "content": chunk}